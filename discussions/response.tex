\documentclass[preprint]{imsart}
%\bibliographystyle{asa}
\usepackage{fullpage}

\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.png,.pdf}
\graphicspath{{3-classifly/}{3-hclust/}{3-manova/}{3-tools/}{4-meifly/}{5-projection-pursuit/}{5-som/}{6-nnet/}}

\usepackage{hyperref}
\usepackage{color}
\definecolor{slateblue}{rgb}{0.07,0.07,0.488}
\hypersetup{colorlinks=true,linkcolor=slateblue,anchorcolor=slateblue,citecolor=slateblue,filecolor=slateblue,urlcolor=slateblue,bookmarksnumbered=true,pdfview=FitB}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\hh}[1]{{\color{ForestGreen} #1}}


\usepackage[small]{caption}
\usepackage{url}
\usepackage[round,sort&compress,sectionbib]{natbib}
\usepackage{amsmath}

\startlocaldefs
\DeclareMathOperator{\Normal}{Normal}
\DeclareMathOperator{\logit}{logit}
\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Authors' Response to Discussants}
\runtitle{Visualizing statistical models}
\begin{aug}
\author{\fnms{Hadley} \snm{Wickham}\corref{}\ead[label=e1]{hadley@rice.edu}},
\author{\fnms{Dianne} \snm{Cook}\ead[label=e2]{dcook@iastate.edu}}
\and
\author{\fnms{Heike} \snm{Hofmann}\ead[label=e3]{hofmann@iastate.edu}}

\affiliation{Rice University}
\address{Department of Statistics MS-138\\6100 Main St\\Houston TX 77081\\ \printead{e1}}

\affiliation{Iowa State University}
\address{Department of Statistics\\2415 Snedecor Hall\\Ames IA 50011-1210\\ \printead{e2}}

\affiliation{Iowa State University}
\address{Department of Statistics\\2413 Snedecor Hall\\Ames IA 50011-1210\\ \printead{e3}}

\end{aug}
\end{frontmatter}

We very much appreciate the insights and additional material provided by the discussants. In the world of big data, visualization becomes so  important on many fronts - communication of information to a broad audience, diagnosis of analytical methods, exploration - but it becomes increasingly nebulous with large volumes of data. Using analytical procedures to process the data can make a problem initially tractable.

Allen et al (2015)  provides an example of the principle "visualizing the process of model fitting" outlined in our paper, successfully applied to big data. In the process of visualizing distributed optimizations, they realized that the same results could be obtained by a single optimization using a range of parameters, which was further validated by theoretical work. It was the visualization that led them to this discovery. They also provide areas in machine learning with big data where "visualizing members of the collection" could be applied to yield better models. The third principle of "visualizing the model in the data space" is where they see less scope for use with big data. They outline several key issues - overplotting of huge numbers of points, distributed data, mixed data types, high-dimension with few samples - which provide challenges for applying the principle. And we agree. We have given some examples for data containing only quantitative variables. So we issue these as challenges to new statistical graphics and data visualization researchers, to advance our methodology so that glimpses of the model in the data space can be made. What is important is to build a synergy of model fitting, and visualization, and where possible show components of the model in some space of the data. The methods described in the various articles written by Simon Urbanek (referenced in main article)  illustrate some starting points.

Leek et al (2015) represent the main stream view of statistics. We agree that the methods described in the paper are very useful for teaching statistical concepts and machine learning algorithms, too. You can find videos and lecture notes at \url{http://streaming.stat.iastate.edu/~dicook/EDA.and.datamining/} to show how to tease apart the results of a random forest model, among many other examples. There are also videos available at \url{https://vimeo.com/user14048736} that have been created to help students understand various multivariate analysis models and tests. We disagree with the commentary in terms of the scope of visualization in the practice of statistics. We believe that visualization needs to be better integrated in all areas of statistical practice. Conceptually, we believe that data analysis and modeling is a cyclical process, as shown in Figure \ref{hadley-diagram} where visualization feeds back mode improvement information back  into the model.  The interplay between modeling and visualization, and the sometimes orthogonal role that visualization takes are described careful in the introduction to \cite{cook:2007} (the pdf of the chapter is available free on the web), and \cite{Ch95}, \cite{CH90}. Attention should also be drawn to data snooping as practiced in the model selection process -- when we return and re-fit a new model technically this is using up degrees of freedom -- and the modeling world has tended to keep this swept under the carpet. Recent work \cite{berk:2013} and unpublished articles and software at \url{http://www-stat.wharton.upenn.edu/~buja/} provide some solutions.

\centerline{\em Hadley, Can you put your usual diagram of the data analysis cycle here?}


Leek et al have missed the point on the all models perspective. Plotting a single scatterplot of living area and number of bedrooms hints at potential collinearity problems for model fitting, but the correlation does not look so strong, and looking at these two variables only negates the interplay between the multiple predictors. Its funny that given their arguments for people having a difficult time perceiving correlation from a plot, that they would advocate the single scatterplot is better than the all models view. The scatterplot itself doesn't tell us how to fix the model. The all models view tells us that the ``best'' models as measured by $R^2$ contain living area, and bedrooms, albeit having a practically difficult to explain negative coefficient for bedrooms. It says that bedrooms are important. Leek et al neglected to describe the next steps for the modeling, which are to partially regress bedrooms on living area and only consider the residuals from this fit as the predictor. Using the fitting the all models in the classroom would help new students better understand the complications arising from multiple predictors.

We have read \cite{Fisher} article describing their study on human perception of statistical significance. We would argue that it is not important if people can accurately report whether a relationship is statistically significant from a single plot. This is what we have a statistical test to do. People tend to respond more closely to practical significance, if the effect size in the relationship is large then people tend to see this. This is important given that statistical significance in the big data world is cheap, we need visualization to help us gauge the practical significance and violations of assumptions for statistical testing. In the third study of \cite{majumder:2013} subjects were asked to detect trend between two variables in the presence of contamination.   A statistical test of the slope would report no significant trend but people do pick up on the trend, because they can see the contamination, and they actually ignore the contamination and report on the relationship between the majority of cases.

Villa-Vialaneix and Ruiz-Gazen (2015) provide many additional examples to support the use of visualization in statistical practice, and like Allen et al point to the additional complexities afforded by data problems, especially networks and non-numeric variables, today. Their approach of averaging projections of a high-dimensional data set produced by multiple starts is innovative, and merits studying in more depth. We would suggest also looking at the projection coefficients from the different starts as a separate data set, much like the all models approach in our paper. By looking the distributions of these coefficients we might learn more about the high-dimensional function (kurtosis projection pursuit index), whether the (local) maxima are primarily are in one location or two or more locations in the data space. Linking between displays of these coefficients and the projected data could provide more detail on what structure in the data was being detected by the function. We agree that it is still cumbersome to get interactive graphics installed and accessible, and this really should be encouraged as a hot research topic for the field of statistics because the power obtained  for data analysis with easy access interactive graphics is huge. Indeed the interactive graphics available in Villa-Vialaneix's sombrero shiny app are a step in this direction.

Hurley (2015) provides some excellent additional examples of algorithms that can be used to organize objects feeding into the visualization, and also to the difficulty in installing interactive graphics software universally. Algorithms that can help organize data are very important and obtain little attention. The dendrogram is a common used approach for displaying the results of a hierarchical clustering, but the commonly used package for organizing the nodes on the horizontal axis is somewhat arbitrary. The DendSer seriation algorithm by Hurley and Oldford should be the default layout for a dendrogram. It requires that someone programs the algorithm and integrates it with the hierarchical clustering package in R, so that users can easily use it. In general, attention to small detail, the last 10\% of polishing out a method is extremely tedious but vitally important.

In summary, we very much appreciate the thoughtful and detailed discussions. This is an exciting time for visualization research, and the next frontier is better integration of modeling and graphics with easily accessible interactive graphics systems. Successfully making in-roads in these areas puts statistics, as a field, back into the forefront of data science.

\bibliographystyle{abbrvnat}
\bibliography{../references}

\end{document}
